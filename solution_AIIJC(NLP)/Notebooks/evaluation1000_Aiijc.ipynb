{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "evaluation1000_Aiijc",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw6oAQGmZjFR"
      },
      "source": [
        "Загрузка библиотек и данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT4Bnmm7auFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34465f47-5a8b-4859-ba58-e97f7f551738"
      },
      "source": [
        "!pip install simpletransformers==0.61.13\n",
        "!pip uninstall transformers\n",
        "!pip install transformers==4.10.0\n",
        "!pip install wikipedia\n",
        "!pip install googledrivedownloader"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simpletransformers==0.61.13\n",
            "  Downloading simpletransformers-0.61.13-py3-none-any.whl (221 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 221 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting tensorboardx\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 36.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 27.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 32.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 35.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 37.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92 kB 38.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 40.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 40.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122 kB 40.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 40.0 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.13.3-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.13) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.13) (4.62.3)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.13) (2.23.0)\n",
            "Collecting wandb>=0.10.32\n",
            "  Downloading wandb-0.12.4-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.13) (2019.12.20)\n",
            "Collecting transformers>=4.2.0\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 37.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.13) (1.1.5)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.0.0-py2.py3-none-any.whl (8.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.13) (1.4.1)\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 37.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from simpletransformers==0.61.13) (1.19.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 28.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.13) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.13) (3.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=4.2.0->simpletransformers==0.61.13) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers>=4.2.0->simpletransformers==0.61.13) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=4.2.0->simpletransformers==0.61.13) (2.4.7)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers==0.61.13) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers==0.61.13) (2.8.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers==0.61.13) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.4.3-py2.py3-none-any.whl (139 kB)\n",
            "\u001b[K     |████████████████████████████████| 139 kB 48.9 MB/s \n",
            "\u001b[?25hCollecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers==0.61.13) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers==0.61.13) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb>=0.10.32->simpletransformers==0.61.13) (5.4.8)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers==0.61.13) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers==0.61.13) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers==0.61.13) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->simpletransformers==0.61.13) (2021.5.30)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb>=0.10.32->simpletransformers==0.61.13) (1.1.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.13) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.13) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->simpletransformers==0.61.13) (3.0.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 28.5 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 50.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.4 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 47.5 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->simpletransformers==0.61.13) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=4.2.0->simpletransformers==0.61.13) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->simpletransformers==0.61.13) (2018.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.2.0->simpletransformers==0.61.13) (1.0.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.13) (0.8.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.13) (5.1.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.13) (0.10.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.13) (1.5.1)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.13) (4.1.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.13) (4.2.4)\n",
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 47.2 MB/s \n",
            "\u001b[?25hCollecting base58\n",
            "  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.0-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 47.2 MB/s \n",
            "\u001b[?25hCollecting watchdog\n",
            "  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->simpletransformers==0.61.13) (7.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers==0.61.13) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers==0.61.13) (0.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers==0.61.13) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->simpletransformers==0.61.13) (2.6.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (5.1.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (7.6.5)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.4.1-py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 45.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (5.3.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.2.0)\n",
            "Collecting ipython<8.0,>=7.23.1\n",
            "  Downloading ipython-7.28.0-py3-none-any.whl (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 40.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.1.3)\n",
            "Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (1.12.3)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.20-py3-none-any.whl (370 kB)\n",
            "\u001b[K     |████████████████████████████████| 370 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (57.4.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.18.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (4.8.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (1.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers==0.61.13) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (4.8.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.12.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (1.8.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (4.1.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers==0.61.13) (0.5.1)\n",
            "Building wheels for collected packages: subprocess32, pathtools, seqeval, blinker\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=9f86fd269a3aea71662ed6f3eb4e52787717abf84f4397cb397142a1004cb690\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=d41e83912cf9ec90f172f768d11963230a200e74e319cbed6ead1a2df01ec42d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=0b74da29ea41883486dc93a4af200a845a5e2503db09860a9b65612a2d704a05\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=a9d7c259482306037c7d3b689b7d200c977db71b7865f6a1b4fd39e47d2dc76b\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built subprocess32 pathtools seqeval blinker\n",
            "Installing collected packages: prompt-toolkit, ipython, ipykernel, multidict, yarl, smmap, async-timeout, pyyaml, gitdb, fsspec, aiohttp, yaspin, xxhash, watchdog, validators, tokenizers, subprocess32, shortuuid, sentry-sdk, sacremoses, pydeck, pathtools, huggingface-hub, GitPython, docker-pycreds, configparser, blinker, base58, wandb, transformers, tensorboardx, streamlit, seqeval, sentencepiece, datasets, simpletransformers\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.20 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.4.1 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.28.0 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.24 aiohttp-3.7.4.post0 async-timeout-3.0.1 base58-2.1.0 blinker-1.4 configparser-5.0.2 datasets-1.13.3 docker-pycreds-0.4.0 fsspec-2021.10.1 gitdb-4.0.7 huggingface-hub-0.0.19 ipykernel-6.4.1 ipython-7.28.0 multidict-5.2.0 pathtools-0.1.2 prompt-toolkit-3.0.20 pydeck-0.7.0 pyyaml-6.0 sacremoses-0.0.46 sentencepiece-0.1.96 sentry-sdk-1.4.3 seqeval-1.2.2 shortuuid-1.0.1 simpletransformers-0.61.13 smmap-4.0.0 streamlit-1.0.0 subprocess32-3.5.4 tensorboardx-2.4 tokenizers-0.10.3 transformers-4.11.3 validators-0.18.2 wandb-0.12.4 watchdog-2.1.6 xxhash-2.0.2 yarl-1.7.0 yaspin-2.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.11.3\n",
            "Uninstalling transformers-4.11.3:\n",
            "  Would remove:\n",
            "    /usr/local/bin/transformers-cli\n",
            "    /usr/local/lib/python3.7/dist-packages/transformers-4.11.3.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/transformers/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled transformers-4.11.3\n",
            "Collecting transformers==4.10.0\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (0.10.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (3.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (0.0.19)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0) (1.15.0)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.10.0\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=9e380a541c716b5644e74a00d25cad44f234df32eea72fbdae68f9fd1d3c3b7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu41Iuxva7Nl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f993734-bdd9-4f21-827f-6b5b36ad234f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "stopwords = stopwords.words(\"english\")\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer() \n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
        "main_tokenizer = RegexpTokenizer(r'\\w+',)\n",
        "sec_tokenizer = RegexpTokenizer(r'\\S+')\n",
        " \n",
        "from sklearn.externals import joblib\n",
        "import numpy as np\n",
        "from google.colab import output\n",
        "import urllib\n",
        "import difflib\n",
        "import wikipedia\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sKeUMata_57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e87b5403-cac4-4350-fd58-a6571c950dc9"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='13Nuwm7BV-4RXI9JqjPTDE9rcdupkKqlF',\n",
        "                                    dest_path='/Data/AIIJC/aiijc_1578_goodFromTrain_pretrained.model')\n",
        "gdd.download_file_from_google_drive(file_id='1Gl_eWLhS-svO6nqn5OqtIbHPfZDn1pM0',\n",
        "                                    dest_path='/Data/AIIJC/newTrain.csv')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 13Nuwm7BV-4RXI9JqjPTDE9rcdupkKqlF into /Data/AIIJC/aiijc_1578_goodFromTrain_pretrained.model... Done.\n",
            "Downloading 1Gl_eWLhS-svO6nqn5OqtIbHPfZDn1pM0 into /Data/AIIJC/newTrain.csv... Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGm0KLQWZyAt"
      },
      "source": [
        "Функции для предобработки текста"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpQ-cVVRbEKl"
      },
      "source": [
        "def normal_form(word): #Получение нормальной формы слова\n",
        "  word = word.lower()\n",
        "  return word\n",
        " \n",
        "def clean_html(html): #Очистка html\n",
        "  soup = BeautifulSoup(BeautifulSoup(html, \"lxml\").text)\n",
        "  return str(soup.body)\n",
        "\n",
        "def get_good_tokens(text): #Выделение ключевых слов\n",
        "  good_tokens = []\n",
        "\n",
        "  for tokens in tokenizer(text)[1]:\n",
        "    for token in tokens:\n",
        "      token = normal_form(token)\n",
        "      if token not in stopwords:\n",
        "        good_tokens.append(token)\n",
        "  return good_tokens\n",
        "\n",
        "def tokenizer(text): #Токенизация текста в обработанные и необработанные токены\n",
        "  raw_tokens = sec_tokenizer.tokenize(text)\n",
        "  clean_tokens = main_tokenizer.tokenize_sents(raw_tokens)\n",
        "  \n",
        "  nClean_tokens = []\n",
        "  for i in range(len(clean_tokens)):\n",
        "    nClean_tokens.append([])\n",
        "    for m in range(len(clean_tokens[i])):\n",
        "      if normal_form(clean_tokens[i][m]) != 's':\n",
        "        nClean_tokens[i].append(normal_form(clean_tokens[i][m]))\n",
        "\n",
        "  return (raw_tokens, nClean_tokens)\n",
        " \n",
        "def similarity(s1, s2): #Нахождение коэффициента схожести между двумя строками\n",
        "  normalized1 = s1.lower()\n",
        "  normalized2 = s2.lower()\n",
        "  matcher = difflib.SequenceMatcher(None, normalized1, normalized2)\n",
        "  return matcher.ratio()\n",
        "\n",
        "def part_extractor(data,question,step,part_length,corrAns=''): #Функция выделения релевантного фрагмента (Текст, вопрос, длинна фрагмента)\n",
        "  good_tokens = get_good_tokens(question)\n",
        "  \n",
        "  tokens = tokenizer(data)\n",
        "  \n",
        "  isIntext = corrAns.lower() in data.lower()\n",
        "    \n",
        "  for i in range(step-(len(tokens[0]) % step)): #Увеличение количества токенов до кратного длины части\n",
        "    tokens[0].append('')\n",
        "    tokens[1].append('')\n",
        " \n",
        "\n",
        "  match_counter = 0 #Счетчик точных совпадений токенов\n",
        "  best_part = '' #Лучшая часть\n",
        "  max_match_qty = 0 #Максимальное количество совпавших токенов\n",
        " \n",
        "  main_clrTokens = tokens[1]\n",
        "  main_tokens = tokens[0]\n",
        "  \n",
        "  for i in range(0,len(tokens[0])-1,part_length): #Нахождение наиболее релевантной части текста\n",
        "    tokens = main_tokens[i:i+part_length-1]\n",
        "    clrTokens = main_clrTokens[i:i+part_length-1]\n",
        "\n",
        "    for good_token in good_tokens:\n",
        "      if in_tokens(good_token,clrTokens):\n",
        "        match_counter += 1\n",
        "\n",
        "    if match_counter > max_match_qty:\n",
        "      max_match_qty = match_counter\n",
        "      best_part = tokens\n",
        "      \n",
        "   \n",
        "    match_counter = 0\n",
        "  \n",
        "  \n",
        "\n",
        "  fin = '' #Восстановление текста\n",
        "  for i in best_part:\n",
        "    fin += (i+' ')\n",
        "\n",
        "  \n",
        "  return fin,isIntext\n",
        "\n",
        "def in_tokens(token,text):\n",
        "  for i in text:\n",
        "    for m in i:\n",
        "      if token == m:\n",
        "        return True\n",
        "  return False"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wvVBYK_brGn"
      },
      "source": [
        "Загрузка данных и модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ9luh4HbFmO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "38c74a24-37cd-46ff-c98e-c356fcfa8f5d"
      },
      "source": [
        "train = pd.read_csv(\"/Data/AIIJC/newTrain.csv\",index_col=0, error_bad_lines=False,sep=\",\")\n",
        "train.predict_answer = ''\n",
        "train"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>link_1</th>\n",
              "      <th>link_2</th>\n",
              "      <th>predict_answer</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the name of the locality that was name...</td>\n",
              "      <td>Antiene</td>\n",
              "      <td>https://en.wikipedia.org/wiki/List_of_cities_a...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Wikipedia:Naming...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Who was the first woman to be selected as a Ti...</td>\n",
              "      <td>Mary Sullivan</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Sylvia_Earle</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Isabella_Bird</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What 15th century friar was a notable writer a...</td>\n",
              "      <td>Kanutus Johannis</td>\n",
              "      <td>https://en.wikipedia.org/wiki/The_Canterbury_T...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Scriptorium</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What does the Unstrut river form?</td>\n",
              "      <td>the border between the Canton of St. Gallen, t...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Unstrut</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Category:Unstrut...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Who was the eighteenth Shah of Shirvan?</td>\n",
              "      <td>Afridun the Martyr</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Ahmad_of_Shirvan</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Afridun_I</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3996</th>\n",
              "      <td>What is the street named after?</td>\n",
              "      <td>Alexandre-Antonin Taché</td>\n",
              "      <td>https://en.wikipedia.org/wiki/List_of_streets_...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Street_or_road_name</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3997</th>\n",
              "      <td>Who was the student of Joseph Beuys?</td>\n",
              "      <td>Anatol Herzfeld</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Joseph_Beuys</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kunstakademie_D%...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3998</th>\n",
              "      <td>What is the name of the technology that has be...</td>\n",
              "      <td>Village Tronic</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Multi-monitor</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Videotelephony</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3999</th>\n",
              "      <td>Where was the 1923–24 Northern Football League...</td>\n",
              "      <td>Manchester United's Gillingham Stadium</td>\n",
              "      <td>https://en.wikipedia.org/wiki/1923%E2%80%9324_...</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Portsmouth_F.C.</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4000</th>\n",
              "      <td>What is the name of Dope Stars' second album?</td>\n",
              "      <td>Gigahearts</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Dope_Stars_Inc.</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Criminal_Intents...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4000 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               question  ... predict_answer\n",
              "id                                                       ...               \n",
              "1     What is the name of the locality that was name...  ...               \n",
              "2     Who was the first woman to be selected as a Ti...  ...               \n",
              "3     What 15th century friar was a notable writer a...  ...               \n",
              "4                     What does the Unstrut river form?  ...               \n",
              "5               Who was the eighteenth Shah of Shirvan?  ...               \n",
              "...                                                 ...  ...            ...\n",
              "3996                    What is the street named after?  ...               \n",
              "3997               Who was the student of Joseph Beuys?  ...               \n",
              "3998  What is the name of the technology that has be...  ...               \n",
              "3999  Where was the 1923–24 Northern Football League...  ...               \n",
              "4000      What is the name of Dope Stars' second album?  ...               \n",
              "\n",
              "[4000 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ezQTOQbHA-"
      },
      "source": [
        "model = joblib.load('/Data/AIIJC/aiijc_1578_goodFromTrain_pretrained.model')\n",
        "\n",
        "model.args.max_seq_length = 512\n",
        "model.args.silent = True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaJ-DMYYb0lA"
      },
      "source": [
        "Оценка модели на первой 1000 вопросов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1FK5V6nbLAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50a65cc-28d7-477b-e0fa-ffb6c8345a35"
      },
      "source": [
        "allCounter = 0 #Счетчик всех вопросов\n",
        "corrCounter = 0 #Счетчик идеальных ответов\n",
        "simCounter = 0 #Счетчик схожести ответов\n",
        "errorCounter = 0 #Счетчик ошибок парсера\n",
        "intext1Counter = 0 #Счетчики нахождения ответа в тексте\n",
        "intext2Counter = 0 \n",
        "inPart1_Counter = 0 #Счетчики нахождения ответа в выделенных фрагментах\n",
        "inPart2_Counter = 0 \n",
        " \n",
        "for i in range(0,1000): #Проверка модели на первой тысяче вопросов тренировочной выборки\n",
        "  question = train.question.iloc[i]\n",
        "  \n",
        "  good_tokens = get_good_tokens(question) #Получаем ключевые слова вопроса\n",
        "\n",
        "  #Загрузка статей википедии\n",
        "  try:\n",
        "    link_1 = train.link_1.iloc[i].replace('https://en.wikipedia.org/wiki/','') #Убераем начало ссылки\n",
        "    link_1 = urllib.parse.unquote(link_1) #Заменяем кривые символы на оригинал\n",
        "    data_1 = wikipedia.page(link_1,auto_suggest=False).content #Парсим страничку вики\n",
        "    data_1 = data_1.replace('\\n',' ')\n",
        "  except:\n",
        "    errorCounter += 1\n",
        "  try:\n",
        "    link_2 = train.link_2.iloc[i].replace('https://en.wikipedia.org/wiki/','') #Убераем начало ссылки\n",
        "    link_2 = urllib.parse.unquote(link_2) #Заменяем кривые символы на оригинал\n",
        "    data_2 = wikipedia.page(link_2,auto_suggest=False).content #Парсим страничку вики\n",
        "    data_2 = data_2.replace('\\n',' ')\n",
        "  except:\n",
        "    errorCounter += 1\n",
        "\n",
        "  #Выделение фрагмента и предсказание по двум статьям\n",
        "  try:\n",
        "    res = part_extractor(data_1,question,16,64,train.answer.iloc[i])\n",
        "    context = res[0]\n",
        "    if res[1]:\n",
        "      intext1Counter += 1\n",
        "    if train.answer.iloc[i].lower() in res[0].lower():\n",
        "      inPart1_Counter += 1\n",
        "  except:\n",
        "    errorCounter += 1\n",
        " \n",
        "  try:\n",
        "    res = part_extractor(data_2,question,16,32,train.answer.iloc[i])\n",
        "    context += ' ' + res[0]\n",
        "    if res[1]:\n",
        "      intext2Counter += 1\n",
        "    if train.answer.iloc[i].lower() in res[0].lower():\n",
        "      inPart2_Counter += 1\n",
        "  except:\n",
        "    errorCounter += 1\n",
        "\n",
        "\n",
        "  predict = model.predict([{'context': context,'qas': [{'id': 0, 'question': question}]}])[0]\n",
        "\n",
        "  #Если ответ не был найден, перебираем прочие длины фрагмента\n",
        "  if predict[0]['answer'][0] == 'empty':\n",
        "    try:\n",
        "      context = part_extractor(data_1,question,16,64)[0]\n",
        "      predict = model.predict([{'context': context,'qas': [{'id': 0, 'question': question}]}])[0]\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  if predict[0]['answer'][0] == 'empty':\n",
        "    try:\n",
        "      context = part_extractor(data_2,question,16,64)[0]\n",
        "      predict = model.predict([{'context': context,'qas': [{'id': 0, 'question': question}]}])[0]\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  if predict[0]['answer'][0] == 'empty':\n",
        "    try:\n",
        "      context = part_extractor(data_1,question,16,128)[0]\n",
        "      predict = model.predict([{'context': context,'qas': [{'id': 0, 'question': question}]}])[0]\n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "  if predict[0]['answer'][0] == 'empty':\n",
        "    try:\n",
        "      context = part_extractor(data_2,question,16,128)[0]\n",
        "      predict = model.predict([{'context': context,'qas': [{'id': 0, 'question': question}]}])[0]\n",
        "    except:\n",
        "      pass\n",
        " \n",
        "  if predict[0]['answer'][0] == 'empty':\n",
        "    try:\n",
        "      context = part_extractor(data_1,question,16,256)[0]\n",
        "      predict = model.predict([{'context': context,'qas': [{'id': 0, 'question': question}]}])[0]\n",
        "    except:\n",
        "      pass\n",
        " \n",
        "  if predict[0]['answer'][0] == 'empty':\n",
        "    try:\n",
        "      context = part_extractor(data_2,question,16,256)[0]\n",
        "      predict = model.predict([{'context': context,'qas': [{'id': 0, 'question': question}]}])[0]\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  #Статистика\n",
        "  allCounter += 1\n",
        "\n",
        "  if str(predict[0]['answer'][0]).lower() == str(train.answer.iloc[i]).lower():\n",
        "    corrCounter += 1\n",
        "    print('Верный ответ - ' + str(train.answer.iloc[i]))\n",
        " \n",
        "  if (i % 5) == 0:\n",
        "    output.clear()\n",
        " \n",
        "  simCounter += similarity(str(predict[0]['answer'][0]).lower(),str(train.answer.iloc[i]).lower())\n",
        " \n",
        "  print('Количество правильных ответов - ' + str(corrCounter) + '/' + str(allCounter))\n",
        " \n",
        "  train.predict_answer.iloc[i] = predict[0]['answer'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество правильных ответов - 385/996\n",
            "Верный ответ - Dublin\n",
            "Количество правильных ответов - 386/997\n",
            "Количество правильных ответов - 386/998\n",
            "Количество правильных ответов - 386/999\n",
            "Количество правильных ответов - 386/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6qP2ySCU5kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae27a83f-8805-426b-af8f-4c3024a10f81"
      },
      "source": [
        "print('Количество идеальных ответов: ' + str(corrCounter))\n",
        "print('Количество ошибок парсинга: ' + str(errorCounter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество идеальных ответов: 386\n",
            "Количество ошибок парсинга: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfuI6actKdeD",
        "outputId": "ac023348-d82f-485d-f420-db12581294d0"
      },
      "source": [
        "print('Средний процент схожести ответов: ' + str(simCounter/i))\n",
        "print('Вероятность правильного ответа: '+ str(corrCounter/allCounter))\n",
        "print('Процент нахождения правильного ответа в первой статье: ' + str(intext1Counter/allCounter))\n",
        "print('Процент нахождения правильного ответа во второй статье: ' + str(intext2Counter/allCounter))\n",
        "print('Процент нахождения ответа в первом фрагменте: ' + str(inPart1_Counter/allCounter))\n",
        "print('Процент нахождения ответа во втором фрагменте: ' + str(inPart2_Counter/allCounter))\n",
        "print('Вероятность успешно выделить фрагмент из первой статьи(Если в ней есть ответ): ' + str(inPart1_Counter/intext1Counter))\n",
        "print('Вероятность успешно выделить фрагмент из второй статьи(Если в ней есть ответ): ' + str(inPart2_Counter/intext2Counter))\n",
        "print('Вероятность успешно ответить при верно выделенном фрагменте: ' + str(corrCounter/((inPart1_Counter+inPart2_Counter*0.2))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Средний процент схожести ответов: 0.5700366564061863\n",
            "Вероятность правильного ответа: 0.386\n",
            "Процент нахождения правильного ответа в первой статье: 0.48\n",
            "Процент нахождения правильного ответа во второй статье: 0.261\n",
            "Процент нахождения ответа в первом фрагменте: 0.369\n",
            "Процент нахождения ответа во втором фрагменте: 0.102\n",
            "Вероятность успешно выделить фрагмент из первой статьи(Если в ней есть ответ): 0.76875\n",
            "Вероятность успешно выделить фрагмент из второй статьи(Если в ней есть ответ): 0.39080459770114945\n",
            "Вероятность успешно ответить при верно выделенном фрагменте: 0.9912686183872625\n"
          ]
        }
      ]
    }
  ]
}